{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e869ab8",
   "metadata": {},
   "source": [
    "# Autoencoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de78412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from models import ConvEncoder, ConvDecoder, ConvAE\n",
    "from training import train_epoch, val_epoch, train_epochs, ft_train_epoch, ft_val_epoch, ft_train_epochs\n",
    "from functions import my_accuracy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from time import sleep\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_param_importances, plot_contour\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e9852",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885e348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset\n",
    "\n",
    "dataset = datasets.MNIST('MNIST', train = True, download = True,\n",
    "                             transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                                 transforms.Normalize(0.0,1.0)\n",
    "                             ]))\n",
    "\n",
    "test_data = datasets.MNIST('MNIST', train = False, download = True,\n",
    "                             transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                                 transforms.Normalize(0.0,1.0)\n",
    "                             ]))\n",
    "\n",
    "# Divide data dataset in train dataset and val dataset\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36460fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_data, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 1, shuffle=True, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84de77b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Mnist\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 4,4\n",
    "\n",
    "\n",
    "for i in range(1,cols * rows+1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f58bb2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b528883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c1b7b",
   "metadata": {},
   "source": [
    "### SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d56c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filename of the net\n",
    "filename = 'AE_sgd.torch'\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### Initialize the two networks\n",
    "parameters = {\n",
    "    \"encoded_space_dim\" : 20,\n",
    "    \"act\" : nn.ReLU,\n",
    "    \"drop_p\" : 0.1\n",
    "}\n",
    "net = ConvAE(parameters)\n",
    "\n",
    "### Move to device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.5, weight_decay = 0.0)\n",
    "\n",
    "# Training \n",
    "max_num_epochs = 20\n",
    "early_stopping = True\n",
    "train_loss_sgd, val_loss_sgd = train_epochs(net, device, train_dataloader, val_dataloader, test_data, loss_function, optimizer, max_num_epochs, early_stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a973ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_sgd, label='Train loss')\n",
    "plt.semilogy(val_loss_sgd, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4906b9",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e247605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvAE(\n",
       "  (encoder): ConvEncoder(\n",
       "    (first_conv): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (second_conv): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (encoder_lin): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=128, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvDecoder(\n",
       "    (decoder_lin): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=128, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
       "    )\n",
       "    (unflatten): Unflatten(dim=1, unflattened_size=(32, 4, 4))\n",
       "    (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "    (first_deconv): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ConvTranspose2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    )\n",
       "    (second_deconv): Sequential(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ConvTranspose2d(16, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Filename of the net\n",
    "filename = 'AE_adam.torch'\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### Initialize the two networks\n",
    "parameters = {\n",
    "    \"encoded_space_dim\" : 20,\n",
    "    \"act\" : nn.ReLU,\n",
    "    \"drop_p\" : 0.1\n",
    "}\n",
    "net = ConvAE(parameters)\n",
    "\n",
    "### Move to device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c5cde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753bbcfe3cc14ab09981596ddcbbf984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1eee9c9b49f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplot_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_loss_adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_progress\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Deep Learning/NNDL/Homework_2/training.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(net, device, train_dataloader, val_dataloader, test_data, loss_function, optimizer, max_num_epochs, early_stopping, plot_progress)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Train an epoch and save losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mmean_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Validate an epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Deep Learning/NNDL/Homework_2/training.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(net, device, dataloader, loss_function, optimizer, noise)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001, weight_decay = 0.0)\n",
    "\n",
    "# Training \n",
    "max_num_epochs = 20\n",
    "early_stopping = True\n",
    "plot_progress = True\n",
    "train_loss_adam, val_loss_adam = train_epochs(net, device, train_dataloader, val_dataloader, test_data, loss_function, optimizer, max_num_epochs, early_stopping, plot_progress )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcdf61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_adam, label='Train loss')\n",
    "plt.semilogy(val_loss_adam, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0c280",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10404dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save network parameters\n",
    "### Save the network state\n",
    "# Save network parameters\n",
    "\n",
    "torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9ffd4",
   "metadata": {},
   "source": [
    "## Hyperparamater optimization\n",
    "### Optuna framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95a02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define objects to be optimized\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    activation_name = trial.suggest_categorical(\"activation\", [\"ReLU\", \"LeakyReLU\"])\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log = True)\n",
    "    \n",
    "    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
    "    drop_p = trial.suggest_float(\"dropout\", 0.0, 1.0)\n",
    "    encoded_space_dim = trial.suggest_int(\"encoded_space_dim\", 2, 30)\n",
    "    \n",
    "    ### Initialize the two networks\n",
    "    parameters = {\n",
    "        \"encoded_space_dim\" : encoded_space_dim,\n",
    "        \"act\" : getattr(nn, activation_name),\n",
    "        \"drop_p\" : drop_p\n",
    "    }\n",
    "    \n",
    "    # Define the model\n",
    "    model = ConvAE(parameters)\n",
    "   \n",
    "    # Define the optimizer\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, momentum = momentum)\n",
    "        \n",
    "    # Define the loss function \n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    max_num_epochs = 10\n",
    "    early_stopping = False\n",
    "    plot_progress = False\n",
    "    train_loss, val_loss, accuracy = train_epochs(model, device, train_dataloader, val_dataloader, test_dataloader, loss_function, optimizer, max_num_epochs, early_stopping, plot_progress)\n",
    "    \n",
    "    # Metric to be minimized is the last validation loss\n",
    "    return val_loss[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9c3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-10 12:01:39,171]\u001b[0m A new study created in memory with name: no-name-d475fd97-e126-469d-8f25-cbd325793430\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28473d56b0594741a183495c03a84d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8d53a9eaff20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cc381f7f832a>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mplot_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Metric to be minimized is the last validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Deep Learning/NNDL/Homework_2/training.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[0;34m(net, device, train_dataloader, val_dataloader, test_data, loss_function, optimizer, max_num_epochs, early_stopping, plot_progress)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Train an epoch and save losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mmean_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Validate an epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Deep Learning/NNDL/Homework_2/training.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(net, device, dataloader, loss_function, optimizer, noise)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials= 20)\n",
    "\n",
    "study.best_params  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe803f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3736542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8022c",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b28907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e98a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaa362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123e724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa8f53ee",
   "metadata": {},
   "source": [
    "## Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f10fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the encoded representation of the test samples\n",
    "labels = []\n",
    "matrix = []\n",
    "for sample in tqdm_notebook(test_data):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img, _  = net(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    matrix.append(encoded_img)\n",
    "    labels.append(label)\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0ff8c",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fc73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular values decomposition\n",
    "pca = PCA(n_components=2, svd_solver='full').fit_transform(matrix)\n",
    "\n",
    "# Create dictionary\n",
    "encoded_samples_pca = []\n",
    "for i in tqdm_notebook(range(pca.shape[0])):\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(pca[i])}\n",
    "    encoded_sample[\"label\"] = labels[i]\n",
    "    encoded_samples_pca.append(encoded_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3984e24",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE projection\n",
    "tsne = TSNE(n_components=2, init='pca').fit_transform(matrix)\n",
    "\n",
    "# Create dictionary\n",
    "encoded_samples_tsne = []\n",
    "for i in tqdm_notebook(range(tsne.shape[0])):\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(tsne[i])}\n",
    "    encoded_sample[\"label\"] = labels[i]\n",
    "    encoded_samples_tsne.append(encoded_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e7b86",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec62230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "df_pca = pd.DataFrame(encoded_samples_pca)\n",
    "df_tsne = pd.DataFrame(encoded_samples_tsne)\n",
    "\n",
    "px.scatter(df_pca, x='Enc. Variable 0', y='Enc. Variable 1', color=df_pca.label.astype(str), opacity=0.7)\n",
    "#px.scatter(df_tsne, x='Enc. Variable 0', y='Enc. Variable 1', color=df_tsne.label.astype(str), opacity=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf946dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct images\n",
    "fig, axs = plt.subplots(3, 2, figsize=(18,9))\n",
    "net.eval()\n",
    "\n",
    "# Get the output of a specific image (the test image at index 0 in this case)\n",
    "img_indx =10\n",
    "img = test_data[img_indx][0].unsqueeze(0).to(device)\n",
    "label = test_data[img_indx][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ , rec_img  = net(img)\n",
    "    \n",
    "# Plot the reconstructed image\n",
    "axs[0,0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[0,0].set_title('Original image: label %d' %label)\n",
    "axs[0,1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[0,1].set_title('Reconstructed image')\n",
    "\n",
    "\n",
    "\n",
    "# Get the output of a specific image (the test image at index 0 in this case)\n",
    "img_indx = 12\n",
    "img = test_data[img_indx][0].unsqueeze(0).to(device)\n",
    "label = test_data[img_indx][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, rec_img  =net(img)\n",
    "    \n",
    "# Plot the reconstructed image\n",
    "axs[1,0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[1,0].set_title('Original image: label %d' %label)\n",
    "axs[1,1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[1,1].set_title('Reconstructed image')\n",
    "\n",
    "\n",
    "# Get the output of a specific image (the test image at index 0 in this case)\n",
    "img_indx = 122\n",
    "img = test_data[img_indx][0].unsqueeze(0).to(device)\n",
    "label = test_data[img_indx][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, rec_img  = net(img)\n",
    "    \n",
    "# Plot the reconstructed image\n",
    "axs[2,0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[2,0].set_title('Original image: label %d' %label)\n",
    "axs[2,1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[2,1].set_title('Reconstructed image')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ee20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate samples from encoded space\n",
    "\n",
    "# Generate a custom sample\n",
    "custom_encoded_sample = np.random.rand(20)\n",
    "encoded_value = torch.tensor(custom_encoded_sample).float().unsqueeze(0).to(device)\n",
    "\n",
    "# Decode sample\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    _, generated_img  = net.(encoded_value)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(generated_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate regular samples from encoded space\n",
    "\n",
    "figure = plt.figure(figsize=(16, 16))\n",
    "cols, rows = 8, 8\n",
    "\n",
    "dim1 = np.linspace(-2,2,cols)\n",
    "dim2 = np.linspace(-2,2,rows)\n",
    "\n",
    "for i in range(cols):\n",
    "    for j in range(rows):\n",
    "        # Create  an encoded sample\n",
    "        encoded_sample = [dim1[i], dim2[j]]\n",
    "        encoded_sample = torch.tensor(encoded_sample).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Decode sample\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_img  = decoder(encoded_sample)\n",
    "        sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "        img, label = train_data[sample_idx]\n",
    "        figure.add_subplot(rows, cols, j*cols+1+i)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(generated_img.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a45c3a",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a99a59c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvAE:\n\tMissing key(s) in state_dict: \"encoder.first_conv.0.weight\", \"encoder.first_conv.0.bias\", \"encoder.first_conv.1.weight\", \"encoder.first_conv.1.bias\", \"encoder.first_conv.1.running_mean\", \"encoder.first_conv.1.running_var\", \"encoder.second_conv.0.weight\", \"encoder.second_conv.0.bias\", \"encoder.second_conv.1.weight\", \"encoder.second_conv.1.bias\", \"encoder.second_conv.1.running_mean\", \"encoder.second_conv.1.running_var\", \"encoder.encoder_lin.0.weight\", \"encoder.encoder_lin.0.bias\", \"encoder.encoder_lin.1.weight\", \"encoder.encoder_lin.1.bias\", \"encoder.encoder_lin.1.running_mean\", \"encoder.encoder_lin.1.running_var\", \"encoder.encoder_lin.4.weight\", \"encoder.encoder_lin.4.bias\", \"decoder.decoder_lin.0.weight\", \"decoder.decoder_lin.0.bias\", \"decoder.decoder_lin.3.weight\", \"decoder.decoder_lin.3.bias\", \"decoder.decoder_lin.3.running_mean\", \"decoder.decoder_lin.3.running_var\", \"decoder.decoder_lin.4.weight\", \"decoder.decoder_lin.4.bias\", \"decoder.first_deconv.2.weight\", \"decoder.first_deconv.2.bias\", \"decoder.first_deconv.2.running_mean\", \"decoder.first_deconv.2.running_var\", \"decoder.first_deconv.3.weight\", \"decoder.first_deconv.3.bias\", \"decoder.second_deconv.2.weight\", \"decoder.second_deconv.2.bias\", \"decoder.second_deconv.2.running_mean\", \"decoder.second_deconv.2.running_var\", \"decoder.second_deconv.3.weight\", \"decoder.second_deconv.3.bias\". \n\tUnexpected key(s) in state_dict: \"first_conv.0.weight\", \"first_conv.0.bias\", \"first_conv.2.weight\", \"first_conv.2.bias\", \"first_conv.2.running_mean\", \"first_conv.2.running_var\", \"first_conv.2.num_batches_tracked\", \"second_conv.0.weight\", \"second_conv.0.bias\", \"second_conv.2.weight\", \"second_conv.2.bias\", \"second_conv.2.running_mean\", \"second_conv.2.running_var\", \"second_conv.2.num_batches_tracked\", \"encoder_lin.0.weight\", \"encoder_lin.0.bias\", \"encoder_lin.2.weight\", \"encoder_lin.2.bias\", \"encoder_lin.2.running_mean\", \"encoder_lin.2.running_var\", \"encoder_lin.2.num_batches_tracked\", \"encoder_lin.4.weight\", \"encoder_lin.4.bias\", \"decoder_lin.0.weight\", \"decoder_lin.0.bias\", \"decoder_lin.2.weight\", \"decoder_lin.2.bias\", \"decoder_lin.2.running_mean\", \"decoder_lin.2.running_var\", \"decoder_lin.2.num_batches_tracked\", \"decoder_lin.4.weight\", \"decoder_lin.4.bias\", \"first_deconv.1.weight\", \"first_deconv.1.bias\", \"first_deconv.1.running_mean\", \"first_deconv.1.running_var\", \"first_deconv.1.num_batches_tracked\", \"first_deconv.3.weight\", \"first_deconv.3.bias\", \"second_deconv.1.weight\", \"second_deconv.1.bias\", \"second_deconv.1.running_mean\", \"second_deconv.1.running_var\", \"second_deconv.1.num_batches_tracked\", \"second_deconv.3.weight\", \"second_deconv.3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0808c60444f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1483\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvAE:\n\tMissing key(s) in state_dict: \"encoder.first_conv.0.weight\", \"encoder.first_conv.0.bias\", \"encoder.first_conv.1.weight\", \"encoder.first_conv.1.bias\", \"encoder.first_conv.1.running_mean\", \"encoder.first_conv.1.running_var\", \"encoder.second_conv.0.weight\", \"encoder.second_conv.0.bias\", \"encoder.second_conv.1.weight\", \"encoder.second_conv.1.bias\", \"encoder.second_conv.1.running_mean\", \"encoder.second_conv.1.running_var\", \"encoder.encoder_lin.0.weight\", \"encoder.encoder_lin.0.bias\", \"encoder.encoder_lin.1.weight\", \"encoder.encoder_lin.1.bias\", \"encoder.encoder_lin.1.running_mean\", \"encoder.encoder_lin.1.running_var\", \"encoder.encoder_lin.4.weight\", \"encoder.encoder_lin.4.bias\", \"decoder.decoder_lin.0.weight\", \"decoder.decoder_lin.0.bias\", \"decoder.decoder_lin.3.weight\", \"decoder.decoder_lin.3.bias\", \"decoder.decoder_lin.3.running_mean\", \"decoder.decoder_lin.3.running_var\", \"decoder.decoder_lin.4.weight\", \"decoder.decoder_lin.4.bias\", \"decoder.first_deconv.2.weight\", \"decoder.first_deconv.2.bias\", \"decoder.first_deconv.2.running_mean\", \"decoder.first_deconv.2.running_var\", \"decoder.first_deconv.3.weight\", \"decoder.first_deconv.3.bias\", \"decoder.second_deconv.2.weight\", \"decoder.second_deconv.2.bias\", \"decoder.second_deconv.2.running_mean\", \"decoder.second_deconv.2.running_var\", \"decoder.second_deconv.3.weight\", \"decoder.second_deconv.3.bias\". \n\tUnexpected key(s) in state_dict: \"first_conv.0.weight\", \"first_conv.0.bias\", \"first_conv.2.weight\", \"first_conv.2.bias\", \"first_conv.2.running_mean\", \"first_conv.2.running_var\", \"first_conv.2.num_batches_tracked\", \"second_conv.0.weight\", \"second_conv.0.bias\", \"second_conv.2.weight\", \"second_conv.2.bias\", \"second_conv.2.running_mean\", \"second_conv.2.running_var\", \"second_conv.2.num_batches_tracked\", \"encoder_lin.0.weight\", \"encoder_lin.0.bias\", \"encoder_lin.2.weight\", \"encoder_lin.2.bias\", \"encoder_lin.2.running_mean\", \"encoder_lin.2.running_var\", \"encoder_lin.2.num_batches_tracked\", \"encoder_lin.4.weight\", \"encoder_lin.4.bias\", \"decoder_lin.0.weight\", \"decoder_lin.0.bias\", \"decoder_lin.2.weight\", \"decoder_lin.2.bias\", \"decoder_lin.2.running_mean\", \"decoder_lin.2.running_var\", \"decoder_lin.2.num_batches_tracked\", \"decoder_lin.4.weight\", \"decoder_lin.4.bias\", \"first_deconv.1.weight\", \"first_deconv.1.bias\", \"first_deconv.1.running_mean\", \"first_deconv.1.running_var\", \"first_deconv.1.num_batches_tracked\", \"first_deconv.3.weight\", \"first_deconv.3.bias\", \"second_deconv.1.weight\", \"second_deconv.1.bias\", \"second_deconv.1.running_mean\", \"second_deconv.1.running_var\", \"second_deconv.1.num_batches_tracked\", \"second_deconv.3.weight\", \"second_deconv.3.bias\". "
     ]
    }
   ],
   "source": [
    "### Reload the network state\n",
    "# Load network parameters\n",
    "filename = \"AE_params.torch\"\n",
    "params_dict = torch.load(filename)\n",
    "\n",
    "\n",
    "### Initialize the two networks\n",
    "parameters = {\n",
    "    \"encoded_space_dim\" : 20,\n",
    "    \"act\" : nn.ReLU,\n",
    "    \"drop_p\" : 0.1\n",
    "}\n",
    "\n",
    "net = ConvAE(parameters)\n",
    "net.load_state_dict(params_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8dff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disallow the update of all the layers by setting the 'requires_grad' parameter of the tensors to 'False'\n",
    "for param_name, param in net.named_parameters():\n",
    "    print(param_name)\n",
    "    print('\\tFreezing update')\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the last layer to match the new reduced problem    \n",
    "num_classes = 10\n",
    "net.encoder.encoder_lin[2] = torch.nn.Linear(128, num_classes)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the training of the last two layers (classifier[4] and classifier[6])\n",
    "for param in net.encoder.encoder_lin[0].parameters():\n",
    "    param.requires_grad = True \n",
    "for param in net.encoder.encoder_lin[2].parameters():\n",
    "    param.requires_grad = True \n",
    "\n",
    "# Check if correct\n",
    "for param_name, param in net.named_parameters():\n",
    "    print('%s \\t- requires_grad=%s' % (param_name, param.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68d535",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7946b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizers\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001, weight_decay = 0.0)\n",
    "\n",
    "### Training epochs\n",
    "max_num_epochs = 20\n",
    "early_stopping = True\n",
    "train_loss_log, val_loss_log, accuracy = ft_train_epochs(net, device, train_dataloader, val_dataloader, test_dataloader, loss_function, optimizer, max_num_epochs, early_stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot losses\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.semilogy(train_loss_log, label='Train loss')\n",
    "ax1.semilogy(val_loss_log, label='Validation loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(accuracy, label = \"Test accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Test accuracy (%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f0026",
   "metadata": {},
   "source": [
    "# Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036500e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the noising function\n",
    "\n",
    "class AddNoise(object):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise with given standard deviation to Mnist images\n",
    "    \"\"\"\n",
    "    def __init__(self, std = 1.):\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        return img+torch.randn_like(img)*self.std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24705a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define noise function\n",
    "noise = AddNoise(std = 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45509b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot image and noised version\n",
    "\n",
    "img_indx = 12\n",
    "img = test_data[img_indx][0]\n",
    "\n",
    "noised_img = noise(img)\n",
    "\n",
    "\n",
    "# Plot the reconstructed image\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "axs[0].imshow(img.squeeze().numpy(), cmap='gist_gray')\n",
    "axs[0].set_title('Original image')\n",
    "axs[1].imshow(noised_img.squeeze().numpy(), cmap='gist_gray')\n",
    "axs[1].set_title('Noised image')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab95fe",
   "metadata": {},
   "source": [
    "## Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### Initialize the two networks\n",
    "encoded_space_dim = 20\n",
    "net = ConvAE(encoded_space_dim=encoded_space_dim)\n",
    "\n",
    "### Move to device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d0c38",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define the optimizers\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001, weight_decay = 0.0)\n",
    "\n",
    "### Training loop with progress bar\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "train_loss_log = []\n",
    "val_loss_log = []\n",
    "\n",
    "pbar = tqdm_notebook(range(num_epochs))\n",
    "\n",
    "for epoch_num in pbar:\n",
    "    \n",
    "    # Train an epoch and save losses\n",
    "    train_epoch_loss = train_epoch(net, device, train_dataloader, loss_function, optimizer_1, noise = noise)\n",
    "    # Validate an epoch\n",
    "    val_epoch_loss = val_epoch(net,  device, val_dataloader, loss_function, noise = noise)\n",
    "    # Compute averages over an epoch\n",
    "    mean_train_loss = np.mean(train_epoch_loss)\n",
    "    mean_val_loss = np.mean(val_epoch_loss)\n",
    "    # Append to plot\n",
    "    train_loss_log.append(mean_train_loss)\n",
    "    val_loss_log.append(mean_val_loss)\n",
    "    pbar.set_description(\"Train loss: %s\" %round(mean_train_loss,3)+\",\"+\"Validation loss %s\" %round(mean_val_loss,3))\n",
    "    \n",
    "    ### Plot progress\n",
    "    # Get the output of a specific image (the test image at index 0 in this case)\n",
    "    img = test_data[0][0].unsqueeze(0).to(device)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        _, rec_img  = net(img)\n",
    "    # Plot the reconstructed image\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "    axs[0].imshow(noise(img).cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    axs[0].set_title('Original noised image')\n",
    "    axs[1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    axs[1].set_title('Reconstructed image (EPOCH %d)' % (epoch_num + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.pause(0.1)\n",
    "    # Save figures\n",
    "    os.makedirs('denoising_autoencoder_progress_%d_features' % encoded_space_dim, exist_ok=True)\n",
    "    fig.savefig('denoising_autoencoder_progress_%d_features/epoch_%d.jpg' % (encoded_space_dim, epoch_num + 1))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    sleep(0.03)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(val_loss_log, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52daf9f",
   "metadata": {},
   "source": [
    "## Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save network parameters\n",
    "### Save the network state\n",
    "# Save network parameters\n",
    "filename = 'DenoisingAE_params.torch'\n",
    "\n",
    "torch.save(net.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed432daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reload the network state\n",
    "# Load network parameters\n",
    "params_dict = torch.load(filename)\n",
    "\n",
    "net.load_state_dict(params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d22cb",
   "metadata": {},
   "source": [
    "## Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c76b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the encoded representation of the test samples\n",
    "encoded_samples = []\n",
    "for sample in tqdm(test_data):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = encoder(noise(img))\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_samples.append(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e02c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "encoded_samples\n",
    "\n",
    "# Scatter plot\n",
    "px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=encoded_samples.label.astype(str), opacity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct images\n",
    "# Get the output of a specific image (the test image at index 0 in this case)\n",
    "img_indx =10\n",
    "img = test_data[img_indx][0].unsqueeze(0).to(device)\n",
    "label = test_data[img_indx][1]\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    rec_img  = decoder(encoder(noise(img)))\n",
    "    \n",
    "# Plot the reconstructed image\n",
    "fig, axs = plt.subplots(3, 2, figsize=(18,9))\n",
    "axs[0,0].imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[0,0].set_title('Original image: label %d' %label)\n",
    "axs[0,1].imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "axs[0,1].set_title('Reconstructed image')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3ff31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
